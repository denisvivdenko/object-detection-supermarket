{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV columns are: image_name,x1,y1,x2,y2,class,image_width,image_height\n",
    "columns = [\"image_name\", \"x1\", \"y1\", \"x2\", \"y2\", \"class_name\", \"image_width\", \"image_height\"]\n",
    "\n",
    "train_annotations = pd.read_csv('../data/SKU110K_fixed/annotations/annotations_train.csv', header=None)\n",
    "train_annotations.columns = columns\n",
    "\n",
    "test_annotations = pd.read_csv('../data/SKU110K_fixed/annotations/annotations_test.csv', header=None)\n",
    "test_annotations.columns = columns\n",
    "\n",
    "val_annotations = pd.read_csv('../data/SKU110K_fixed/annotations/annotations_val.csv', header=None)\n",
    "val_annotations.columns = columns\n",
    "\n",
    "dataset = {\n",
    "    \"train\": train_annotations,\n",
    "    \"test\": test_annotations,\n",
    "    \"val\": val_annotations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>class_name</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>208</td>\n",
       "      <td>537</td>\n",
       "      <td>422</td>\n",
       "      <td>814</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>1268</td>\n",
       "      <td>1923</td>\n",
       "      <td>1365</td>\n",
       "      <td>2209</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>1135</td>\n",
       "      <td>2074</td>\n",
       "      <td>1261</td>\n",
       "      <td>2166</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>1045</td>\n",
       "      <td>2085</td>\n",
       "      <td>1122</td>\n",
       "      <td>2258</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>976</td>\n",
       "      <td>2036</td>\n",
       "      <td>1040</td>\n",
       "      <td>2177</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_name    x1    y1    x2    y2 class_name  image_width  image_height\n",
       "0  train_0.jpg   208   537   422   814     object         3024          3024\n",
       "1  train_0.jpg  1268  1923  1365  2209     object         3024          3024\n",
       "2  train_0.jpg  1135  2074  1261  2166     object         3024          3024\n",
       "3  train_0.jpg  1045  2085  1122  2258     object         3024          3024\n",
       "4  train_0.jpg   976  2036  1040  2177     object         3024          3024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.55000e+02, 1.18000e+02, 6.08670e+04, 7.94870e+04, 2.40748e+05,\n",
       "        5.31492e+05, 1.90140e+04, 2.55197e+05, 1.44720e+04, 6.43200e+03]),\n",
       " array([ 640. , 1107.2, 1574.4, 2041.6, 2508.8, 2976. , 3443.2, 3910.4,\n",
       "        4377.6, 4844.8, 5312. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_annotations.image_width)\n",
    "plt.hist(train_annotations.image_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1208482 entries, 0 to 1208481\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count    Dtype \n",
      "---  ------        --------------    ----- \n",
      " 0   image_name    1208482 non-null  object\n",
      " 1   x1            1208482 non-null  int64 \n",
      " 2   y1            1208482 non-null  int64 \n",
      " 3   x2            1208482 non-null  int64 \n",
      " 4   y2            1208482 non-null  int64 \n",
      " 5   class_name    1208482 non-null  object\n",
      " 6   image_width   1208482 non-null  int64 \n",
      " 7   image_height  1208482 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 73.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_annotations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_289855/3967147170.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_annotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/object-detection-supermarket/venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6200\u001b[0m         ):\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'object'"
     ]
    }
   ],
   "source": [
    "train_annotations.object.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/denys/object-detection-supermarket/notebooks/../data/examples/001.jpg: 448x640 (no detections), 76.9ms\n",
      "Speed: 2.2ms preprocess, 76.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(f'../models/yolov8n.pt')\n",
    "results = model.predict(source='../data/examples/001.jpg', conf=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'Boxes.boxes' is deprecated. Use 'Boxes.data' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "boxes: tensor([], size=(0, 6))\n",
       "cls: tensor([])\n",
       "conf: tensor([])\n",
       "data: tensor([], size=(0, 6))\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (2396, 3594)\n",
       "shape: torch.Size([0, 6])\n",
       "xywh: tensor([], size=(0, 4))\n",
       "xywhn: tensor([], size=(0, 4))\n",
       "xyxy: tensor([], size=(0, 4))\n",
       "xyxyn: tensor([], size=(0, 4))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>class_name</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>208</td>\n",
       "      <td>537</td>\n",
       "      <td>422</td>\n",
       "      <td>814</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>1268</td>\n",
       "      <td>1923</td>\n",
       "      <td>1365</td>\n",
       "      <td>2209</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>1135</td>\n",
       "      <td>2074</td>\n",
       "      <td>1261</td>\n",
       "      <td>2166</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>1045</td>\n",
       "      <td>2085</td>\n",
       "      <td>1122</td>\n",
       "      <td>2258</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>976</td>\n",
       "      <td>2036</td>\n",
       "      <td>1040</td>\n",
       "      <td>2177</td>\n",
       "      <td>object</td>\n",
       "      <td>3024</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_name    x1    y1    x2    y2 class_name  image_width  image_height\n",
       "0  train_0.jpg   208   537   422   814     object         3024          3024\n",
       "1  train_0.jpg  1268  1923  1365  2209     object         3024          3024\n",
       "2  train_0.jpg  1135  2074  1261  2166     object         3024          3024\n",
       "3  train_0.jpg  1045  2085  1122  2258     object         3024          3024\n",
       "4  train_0.jpg   976  2036  1040  2177     object         3024          3024"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13317"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_annotations[\"x2\"] > train_annotations[\"image_width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = {class_name: i for i, class_name in enumerate(pd.concat([d for d in dataset.values()])[\"class_name\"].unique())}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 172/8219 [00:55<43:13,  3.10it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 172\u001b[0m\n\u001b[1;32m    168\u001b[0m                     \u001b[39mfor\u001b[39;00m class_name, bbox \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(augmented_image_metadata[\u001b[39m\"\u001b[39m\u001b[39mclasses\u001b[39m\u001b[39m\"\u001b[39m], augmented_image_metadata[\u001b[39m\"\u001b[39m\u001b[39mbboxes\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m    169\u001b[0m                         f\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(bbox)]) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m prepare_dataset(params, dataset, \u001b[39m\"\u001b[39;49m\u001b[39m../data/dataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m../data/SKU110K_fixed/images\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[38], line 148\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(params, datasets, data_path, images_dir)\u001b[0m\n\u001b[1;32m    146\u001b[0m random\u001b[39m.\u001b[39mseed(new_seed)\n\u001b[1;32m    147\u001b[0m augmenter \u001b[39m=\u001b[39m get_augmentations()\n\u001b[0;32m--> 148\u001b[0m augmentation \u001b[39m=\u001b[39m augmenter(image\u001b[39m=\u001b[39;49mimage, bboxes\u001b[39m=\u001b[39;49mbboxes, labels\u001b[39m=\u001b[39;49mclasses)\n\u001b[1;32m    149\u001b[0m augmented_image \u001b[39m=\u001b[39m augmentation[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    150\u001b[0m augmented_bboxes \u001b[39m=\u001b[39m augmentation[\u001b[39m\"\u001b[39m\u001b[39mbboxes\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/object-detection-supermarket/venv/lib/python3.10/site-packages/albumentations/core/composition.py:454\u001b[0m, in \u001b[0;36mReplayCompose.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, force_apply: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mDict[\u001b[39mstr\u001b[39m, typing\u001b[39m.\u001b[39mAny]:\n\u001b[1;32m    453\u001b[0m     kwargs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_key] \u001b[39m=\u001b[39m defaultdict(\u001b[39mdict\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(ReplayCompose, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(force_apply\u001b[39m=\u001b[39;49mforce_apply, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    455\u001b[0m     serialized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_dict_with_id()\n\u001b[1;32m    456\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_with_params(serialized, result[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_key])\n",
      "File \u001b[0;32m~/object-detection-supermarket/venv/lib/python3.10/site-packages/albumentations/core/composition.py:217\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    214\u001b[0m data \u001b[39m=\u001b[39m Compose\u001b[39m.\u001b[39m_make_targets_contiguous(data)  \u001b[39m# ensure output targets are contiguous\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m--> 217\u001b[0m     p\u001b[39m.\u001b[39;49mpostprocess(data)\n\u001b[1;32m    219\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/object-detection-supermarket/venv/lib/python3.10/site-packages/albumentations/core/utils.py:72\u001b[0m, in \u001b[0;36mDataProcessor.postprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     69\u001b[0m rows, cols \u001b[39m=\u001b[39m get_shape(data[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     71\u001b[0m \u001b[39mfor\u001b[39;00m data_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_fields:\n\u001b[0;32m---> 72\u001b[0m     data[data_name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter(data[data_name], rows, cols)\n\u001b[1;32m     73\u001b[0m     data[data_name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_and_convert(data[data_name], rows, cols, direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfrom\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremove_label_fields_from_data(data)\n",
      "File \u001b[0;32m~/object-detection-supermarket/venv/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:125\u001b[0m, in \u001b[0;36mBboxProcessor.filter\u001b[0;34m(self, data, rows, cols)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mself\u001b[39m, data: Sequence, rows: \u001b[39mint\u001b[39m, cols: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams: BboxParams\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m filter_bboxes(\n\u001b[1;32m    126\u001b[0m         data,\n\u001b[1;32m    127\u001b[0m         rows,\n\u001b[1;32m    128\u001b[0m         cols,\n\u001b[1;32m    129\u001b[0m         min_area\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mmin_area,\n\u001b[1;32m    130\u001b[0m         min_visibility\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mmin_visibility,\n\u001b[1;32m    131\u001b[0m         min_width\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mmin_width,\n\u001b[1;32m    132\u001b[0m         min_height\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams\u001b[39m.\u001b[39;49mmin_height,\n\u001b[1;32m    133\u001b[0m     )\n",
      "File \u001b[0;32m~/object-detection-supermarket/venv/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:485\u001b[0m, in \u001b[0;36mfilter_bboxes\u001b[0;34m(bboxes, rows, cols, min_area, min_visibility, min_width, min_height)\u001b[0m\n\u001b[1;32m    482\u001b[0m clipped_box_area \u001b[39m=\u001b[39m calculate_bbox_area(bbox, rows, cols)\n\u001b[1;32m    484\u001b[0m \u001b[39m# Calculate width and height of the clipped bounding box.\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m x_min, y_min, x_max, y_max \u001b[39m=\u001b[39m denormalize_bbox(bbox, rows, cols)[:\u001b[39m4\u001b[39m]\n\u001b[1;32m    486\u001b[0m clipped_width, clipped_height \u001b[39m=\u001b[39m x_max \u001b[39m-\u001b[39m x_min, y_max \u001b[39m-\u001b[39m y_min\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    489\u001b[0m     clipped_box_area \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# to ensure transformed_box_area!=0 and to handle min_area=0 or min_visibility=0\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[39mand\u001b[39;00m clipped_box_area \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_area\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[39mand\u001b[39;00m clipped_height \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_height\n\u001b[1;32m    494\u001b[0m ):\n",
      "File \u001b[0;32m~/object-detection-supermarket/venv/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:176\u001b[0m, in \u001b[0;36mdenormalize_bbox\u001b[0;34m(bbox, rows, cols)\u001b[0m\n\u001b[1;32m    171\u001b[0m     y_min, y_max \u001b[39m=\u001b[39m y_min \u001b[39m/\u001b[39m rows, y_max \u001b[39m/\u001b[39m rows\n\u001b[1;32m    173\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(BoxType, (x_min, y_min, x_max, y_max) \u001b[39m+\u001b[39m tail)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdenormalize_bbox\u001b[39m(bbox: TBox, rows: \u001b[39mint\u001b[39m, cols: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TBox:\n\u001b[1;32m    177\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Denormalize coordinates of a bounding box. Multiply x-coordinates by image width and y-coordinates\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m    by image height. This is an inverse operation for :func:`~albumentations.augmentations.bbox.normalize_bbox`.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     tail: Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/object-detection-supermarket/venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_temp, \u001b[39m'\u001b[39m\u001b[39m_is_stopped\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_temp, \u001b[39m'\u001b[39m\u001b[39m_Thread__stopped\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "import random\n",
    "from src.utils.load_params import load_params\n",
    "\n",
    "params = load_params(\"../params.yaml\")\n",
    "\n",
    "def clear_directory(path: str) -> None:\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)  # Remove files and symbolic links\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)  # Remove subdirectories\n",
    "\n",
    "\n",
    "def get_augmentations() -> A.ReplayCompose:\n",
    "    return A.ReplayCompose([\n",
    "        A.OneOf([\n",
    "            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.9),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9),\n",
    "        ], p=0.9),\n",
    "        A.ToGray(p=0.01),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Transpose(p=0.1),\n",
    "        A.ImageCompression(quality_lower=85, quality_upper=95, p=0.2),\n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=3, p=1.0),\n",
    "            A.MedianBlur(blur_limit=3, p=1.0)\n",
    "        ], p=0.1),\n",
    "        A.Resize(height=416, width=416, p=1),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['labels'], min_visibility=0.1), p=1)\n",
    "\n",
    "\n",
    "def convert_boxes_to_yolo_format(x1: int, y1: int, x2: int, y2: int, image_width: int, image_height: int) -> str:\n",
    "    \"\"\"\n",
    "        Converts the bounding box coordinates to yolo format\n",
    "\n",
    "        returns: str \"x_center_n y_center_n bbox_width_n bbox_height_n\"\n",
    "    \"\"\"\n",
    "\n",
    "    x2 = min(x2, image_width)\n",
    "    y2 = min(y2, image_height)\n",
    "\n",
    "    x_center = (x1 + x2) / 2\n",
    "    y_center = (y1 + y2) / 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "\n",
    "    # if x2 > image_width or y2 > image_height:\n",
    "\n",
    "    #     print(f\"Out of bound box: {x1, y1, x2, y2} for image of size: {image_width, image_height}\")\n",
    "\n",
    "    # if x1 > x2 or y1 > y2:\n",
    "    #     raise ValueError(\"x1 > x2 or y1 > y2\")\n",
    "\n",
    "    x_center_n = x_center / image_width\n",
    "    y_center_n = y_center / image_height\n",
    "    width_n = width / image_width\n",
    "    height_n = height / image_height\n",
    "\n",
    "    x_center_n = min(max(x_center_n, 0), 1)\n",
    "    y_center_n = min(max(y_center_n, 0), 1)\n",
    "    width_n = min(max(width_n, 0), 1)\n",
    "    height_n = min(max(height_n, 0), 1)\n",
    "    \n",
    "    return {\n",
    "        \"x_center_n\": x_center_n,\n",
    "        \"y_center_n\": y_center_n,\n",
    "        \"width_n\": width_n,\n",
    "        \"height_n\": height_n\n",
    "    } \n",
    "\n",
    "\n",
    "def prepare_dataset(params, datasets: pd.DataFrame, data_path: str, images_dir: str) -> None:\n",
    "    \"\"\"\n",
    "        datasets: dict\n",
    "            {\n",
    "                \"train\": ...,\n",
    "                \"test\": ...,\n",
    "                \"val\": ...\n",
    "            }\n",
    "\n",
    "        Creates this data structure\n",
    "\n",
    "        dataset\n",
    "            - images\n",
    "                - train\n",
    "                    - 001.jpg\n",
    "                - test\n",
    "                    - 001.jpg\n",
    "                - val\n",
    "                    - 001.jpg\n",
    "            - labels\n",
    "                - train\n",
    "                    - 001.txt\n",
    "                - test\n",
    "                    - 001.txt\n",
    "                - val\n",
    "                    - 001.txt\n",
    "\n",
    "        YOLO expects this format x_center_n, y_center_n bbox_width_n, bbox_height_n\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_path)\n",
    "    images_dir = Path(images_dir)\n",
    "\n",
    "    if os.path.exists(data_dir):\n",
    "        shutil.rmtree(data_dir)\n",
    "    for split in datasets:\n",
    "        os.makedirs(data_dir / f\"images/{split}\", exist_ok=True)\n",
    "        os.makedirs(data_dir / f\"labels/{split}\", exist_ok=True)\n",
    "\n",
    "    classes_dict = {class_name: i for i, class_name in enumerate(pd.concat([d for d in dataset.values()])[\"class_name\"].unique())}\n",
    "    for split in datasets:\n",
    "        for _, image_metadata in tqdm(datasets[split].groupby(\"image_name\")):\n",
    "            image_name =  image_metadata[\"image_name\"].values[0]\n",
    "            image_fpath = images_dir / image_name\n",
    "\n",
    "            new_image_dir = data_dir / \"images\" / split\n",
    "            new_label_dir = data_dir / \"labels\" / split\n",
    "\n",
    "            \n",
    "            image = Image.open(image_fpath)\n",
    "            image = np.array(image)\n",
    "            bboxes = [convert_boxes_to_yolo_format(box.x1, box.y1, box.x2, box.y2, box.image_width, box.image_height).values() for _, box in image_metadata.iterrows()]\n",
    "            classes = [classes_dict[box[\"class_name\"]] for _, box in image_metadata.iterrows()]\n",
    "            base_random_state = params[\"base\"][\"random_state\"]\n",
    "\n",
    "            augmentations = [{\n",
    "                \"image\": image,\n",
    "                \"bboxes\": bboxes,\n",
    "                \"classes\": classes,\n",
    "                f\"fname\": f\"{image_name.split('.')[0]}_{0}\"\n",
    "            }]\n",
    "            \n",
    "            for i in range(1, params[\"yolo_preprocessing\"][\"n_augmentations\"] + 1):\n",
    "                new_seed = base_random_state + i\n",
    "                np.random.seed(new_seed)\n",
    "                random.seed(new_seed)\n",
    "                augmenter = get_augmentations()\n",
    "                augmentation = augmenter(image=image, bboxes=bboxes, labels=classes)\n",
    "                augmented_image = augmentation[\"image\"]\n",
    "                augmented_bboxes = augmentation[\"bboxes\"]\n",
    "\n",
    "                augmentations.append({\n",
    "                    \"image\": augmented_image, \n",
    "                    \"bboxes\": augmented_bboxes,\n",
    "                    \"classes\": classes,\n",
    "                    f\"fname\": f\"{image_name.split('.')[0]}_{i}\"\n",
    "                })\n",
    "                \n",
    "            np.random.seed(base_random_state)\n",
    "            random.seed(base_random_state)\n",
    "            \n",
    "            for augmented_image_metadata in augmentations:\n",
    "                augmented_image = Image.fromarray(augmented_image_metadata[\"image\"])\n",
    "                augmented_image.save(new_image_dir / f'{augmented_image_metadata[\"fname\"]}.jpg')\n",
    "\n",
    "                with open(new_label_dir / f'{augmented_image_metadata[\"fname\"]}.txt', \"a\") as f:\n",
    "                    for class_name, bbox in zip(augmented_image_metadata[\"classes\"], augmented_image_metadata[\"bboxes\"]):\n",
    "                        f.write(f\"{class_name} \" + ' '.join([str(v) for v in list(bbox)]) + \"\\n\")\n",
    "\n",
    "\n",
    "prepare_dataset(params, dataset, \"../data/dataset\", \"../data/SKU110K_fixed/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
